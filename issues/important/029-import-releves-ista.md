# Issue #029: Import Relev√©s ISTA (Historique Consommations)

**Priorit√©**: Moyenne
**Effort estim√©**: 6-8h
**Phase**: Phase 2 (K3s - Automation & Community)
**D√©pendances**: Issue #030 (optionnel - int√©gration IoT pour validation crois√©e)
**Labels**: `feature`, `automation`, `energy`, `belgium`

---

## üìã Contexte

**ISTA** est le leader europ√©en du sous-comptage et de la r√©partition des frais de chauffage et d'eau en immeubles collectifs. En Belgique, ISTA √©quipe une grande partie des copropri√©t√©s avec des compteurs individuels pour:
- Eau froide et chaude
- Chauffage (r√©partiteurs de frais de chauffage - RFC)
- Calories consomm√©es

Les relev√©s ISTA sont fournis annuellement (ou semestriellement) sous forme de:
- **Fichiers CSV** (export standard)
- **Fichiers XML** (export d√©taill√© avec m√©tadonn√©es)
- **PDF** (rapports visuels non-structur√©s)

**Objectif**: Permettre l'import automatique des relev√©s ISTA dans KoproGo pour:
1. **Historiser** les consommations par unit√© et par p√©riode
2. **Analyser** les tendances de consommation
3. **D√©tecter** les anomalies (surconsommation, fuites)
4. **Comparer** avec les factures fournisseurs (eau, gaz, √©lectricit√©)
5. **Pr√©parer** les d√©comptes individuels de charges

---

## üéØ Objectifs

### Fonctionnels
- ‚úÖ Importer fichiers CSV/XML ISTA avec validation de format
- ‚úÖ Mapper automatiquement les compteurs ISTA aux unit√©s (Unit) via r√©f√©rence cadastrale ou num√©ro d'appartement
- ‚úÖ Stocker l'historique des relev√©s avec horodatage et m√©tadonn√©es
- ‚úÖ Calculer les consommations par p√©riode (delta entre deux relev√©s)
- ‚úÖ G√©n√©rer des rapports d'analyse de consommation (par unit√©, par b√¢timent, par type)
- ‚úÖ D√©tecter les anomalies (variation > 30% entre p√©riodes, valeurs aberrantes)
- ‚úÖ Exporter les donn√©es vers Excel pour comptabilit√©

### Techniques
- ‚úÖ Support multi-formats (CSV, XML) avec parsers extensibles
- ‚úÖ Transaction atomique pour import batch (tout ou rien)
- ‚úÖ Logs d'audit d√©taill√©s (fichier import√©, utilisateur, timestamp, nombre de lignes)
- ‚úÖ Validation stricte des donn√©es (dates, valeurs num√©riques, unit√©s de mesure)
- ‚úÖ D√©duplication automatique (m√™me fichier import√© 2 fois = rejet)

---

## üèóÔ∏è Architecture Technique

### 1. Nouvelles Entit√©s Domain

#### `ISTAReading` (Relev√© ISTA)
```rust
use chrono::{DateTime, Utc};
use uuid::Uuid;

/// Repr√©sente un relev√© de compteur ISTA import√©.
/// Chaque relev√© correspond √† une ligne dans le fichier CSV/XML.
pub struct ISTAReading {
    pub id: Uuid,
    pub organization_id: Uuid,
    pub building_id: Uuid,
    pub unit_id: Option<Uuid>, // None si mapping pas encore effectu√©
    pub import_batch_id: Uuid, // R√©f√©rence au batch d'import

    // Identification du compteur
    pub meter_number: String, // Num√©ro du compteur ISTA (ex: "12345678")
    pub meter_type: MeterType, // ColdWater, HotWater, Heating, Calories
    pub meter_location: Option<String>, // "Cuisine", "Salle de bain", etc.

    // Donn√©es de relev√©
    pub reading_date: DateTime<Utc>, // Date du relev√©
    pub reading_value: f64, // Valeur du compteur (ex: 1234.5 m¬≥)
    pub previous_reading_value: Option<f64>, // Valeur pr√©c√©dente (si fournie par ISTA)
    pub consumption: Option<f64>, // Consommation calcul√©e (reading_value - previous)
    pub unit_of_measure: String, // "m¬≥", "kWh", "unit√©s RFC", etc.

    // M√©tadonn√©es ISTA
    pub ista_contract_number: Option<String>, // Num√©ro de contrat ISTA
    pub billing_period_start: Option<DateTime<Utc>>,
    pub billing_period_end: Option<DateTime<Utc>>,
    pub estimated: bool, // true si relev√© estim√© (non r√©el)

    // Statut
    pub mapped_to_unit: bool, // true si unit_id a √©t√© mapp√© avec succ√®s
    pub anomaly_detected: bool, // true si anomalie d√©tect√©e
    pub anomaly_reason: Option<String>, // Description de l'anomalie

    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum MeterType {
    ColdWater,    // Eau froide
    HotWater,     // Eau chaude sanitaire
    Heating,      // Chauffage (RFC - R√©partiteur de Frais de Chauffage)
    Calories,     // Calories (compteur thermique)
}

impl ISTAReading {
    pub fn new(
        organization_id: Uuid,
        building_id: Uuid,
        import_batch_id: Uuid,
        meter_number: String,
        meter_type: MeterType,
        reading_date: DateTime<Utc>,
        reading_value: f64,
        unit_of_measure: String,
        estimated: bool,
    ) -> Result<Self, String> {
        // Validations
        if meter_number.trim().is_empty() {
            return Err("Meter number cannot be empty".to_string());
        }
        if reading_value < 0.0 {
            return Err("Reading value cannot be negative".to_string());
        }

        Ok(Self {
            id: Uuid::new_v4(),
            organization_id,
            building_id,
            unit_id: None,
            import_batch_id,
            meter_number,
            meter_type,
            meter_location: None,
            reading_date,
            reading_value,
            previous_reading_value: None,
            consumption: None,
            unit_of_measure,
            ista_contract_number: None,
            billing_period_start: None,
            billing_period_end: None,
            estimated,
            mapped_to_unit: false,
            anomaly_detected: false,
            anomaly_reason: None,
            created_at: Utc::now(),
            updated_at: Utc::now(),
        })
    }

    /// Calcule la consommation si previous_reading_value est disponible
    pub fn calculate_consumption(&mut self) {
        if let Some(prev) = self.previous_reading_value {
            self.consumption = Some((self.reading_value - prev).max(0.0));
        }
    }

    /// D√©tecte une anomalie de consommation (> 30% de variation)
    pub fn detect_anomaly(&mut self, avg_consumption: f64) {
        if let Some(consumption) = self.consumption {
            if consumption > avg_consumption * 1.3 || consumption < avg_consumption * 0.3 {
                self.anomaly_detected = true;
                self.anomaly_reason = Some(format!(
                    "Variation anormale: {} {} vs moyenne {} {}",
                    consumption, self.unit_of_measure, avg_consumption, self.unit_of_measure
                ));
            }
        }
    }
}
```

#### `ISTAImportBatch` (Batch d'import)
```rust
/// Repr√©sente un batch d'import de fichier ISTA.
pub struct ISTAImportBatch {
    pub id: Uuid,
    pub organization_id: Uuid,
    pub building_id: Uuid,
    pub imported_by: Uuid, // User ID

    // Fichier import√©
    pub file_name: String,
    pub file_size: i64, // Taille en bytes
    pub file_format: FileFormat, // CSV, XML
    pub file_hash: String, // SHA-256 pour d√©duplication

    // R√©sultats de l'import
    pub total_rows: i32,
    pub successful_rows: i32,
    pub failed_rows: i32,
    pub duplicate_rows: i32,
    pub warnings: Vec<String>, // Avertissements non-bloquants
    pub errors: Vec<String>, // Erreurs de parsing

    // Statut
    pub status: ImportStatus, // Pending, Processing, Completed, Failed

    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum FileFormat {
    CSV,
    XML,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ImportStatus {
    Pending,
    Processing,
    Completed,
    Failed,
}
```

#### `MeterUnitMapping` (Mapping compteur ‚Üí unit√©)
```rust
/// Configuration du mapping entre compteurs ISTA et unit√©s.
/// Permet l'auto-mapping lors des futurs imports.
pub struct MeterUnitMapping {
    pub id: Uuid,
    pub organization_id: Uuid,
    pub building_id: Uuid,
    pub unit_id: Uuid,

    pub meter_number: String, // Num√©ro du compteur ISTA
    pub meter_type: MeterType,
    pub meter_location: Option<String>,

    // Mapping alternatif (si num√©ro compteur change)
    pub fallback_reference: Option<String>, // Ex: cadastre, num√©ro d'appartement

    pub active: bool, // false si compteur remplac√©
    pub created_at: DateTime<Utc>,
    pub deactivated_at: Option<DateTime<Utc>>,
}
```

---

### 2. Parsers (Infrastructure)

#### `ISTACSVParser`
```rust
// backend/src/infrastructure/ista/parsers/csv_parser.rs

use csv::ReaderBuilder;
use chrono::NaiveDate;

pub struct ISTACSVParser;

impl ISTACSVParser {
    /// Parse un fichier CSV ISTA standard.
    /// Format attendu (colonnes):
    /// - meter_number: Num√©ro compteur
    /// - meter_type: Type (water_cold, water_hot, heating, calories)
    /// - location: Emplacement (optionnel)
    /// - reading_date: Date relev√© (DD/MM/YYYY)
    /// - reading_value: Valeur
    /// - previous_value: Valeur pr√©c√©dente (optionnel)
    /// - unit: Unit√© (m¬≥, kWh, etc.)
    /// - estimated: Oui/Non
    pub async fn parse(
        &self,
        file_content: &[u8],
        building_id: Uuid,
        organization_id: Uuid,
    ) -> Result<Vec<ISTAReading>, String> {
        let mut reader = ReaderBuilder::new()
            .delimiter(b';') // ISTA utilise souvent ';' en Europe
            .from_reader(file_content);

        let mut readings = Vec::new();

        for (idx, result) in reader.records().enumerate() {
            let record = result.map_err(|e| format!("CSV parsing error at line {}: {}", idx + 1, e))?;

            // Validation et parsing
            let meter_number = record.get(0)
                .ok_or_else(|| format!("Missing meter_number at line {}", idx + 1))?
                .to_string();

            let meter_type = Self::parse_meter_type(
                record.get(1).unwrap_or("unknown")
            )?;

            let reading_date = Self::parse_date(
                record.get(3).unwrap_or("")
            )?;

            let reading_value: f64 = record.get(4)
                .ok_or_else(|| format!("Missing reading_value at line {}", idx + 1))?
                .parse()
                .map_err(|_| format!("Invalid reading_value at line {}", idx + 1))?;

            let previous_value: Option<f64> = record.get(5)
                .and_then(|s| s.parse().ok());

            let unit = record.get(6).unwrap_or("m¬≥").to_string();

            let estimated = record.get(7).unwrap_or("Non") == "Oui";

            let mut reading = ISTAReading::new(
                organization_id,
                building_id,
                Uuid::new_v4(), // batch_id sera assign√© plus tard
                meter_number,
                meter_type,
                reading_date.and_hms_opt(0, 0, 0).unwrap().and_utc(),
                reading_value,
                unit,
                estimated,
            )?;

            reading.previous_reading_value = previous_value;
            reading.meter_location = record.get(2).map(|s| s.to_string());
            reading.calculate_consumption();

            readings.push(reading);
        }

        Ok(readings)
    }

    fn parse_meter_type(s: &str) -> Result<MeterType, String> {
        match s.to_lowercase().as_str() {
            "water_cold" | "eau_froide" | "cold_water" => Ok(MeterType::ColdWater),
            "water_hot" | "eau_chaude" | "hot_water" => Ok(MeterType::HotWater),
            "heating" | "chauffage" | "rfc" => Ok(MeterType::Heating),
            "calories" | "thermique" => Ok(MeterType::Calories),
            _ => Err(format!("Unknown meter type: {}", s)),
        }
    }

    fn parse_date(s: &str) -> Result<NaiveDate, String> {
        // Support formats: DD/MM/YYYY, YYYY-MM-DD
        NaiveDate::parse_from_str(s, "%d/%m/%Y")
            .or_else(|_| NaiveDate::parse_from_str(s, "%Y-%m-%d"))
            .map_err(|_| format!("Invalid date format: {}", s))
    }
}
```

#### `ISTAXMLParser` (similaire pour XML)
```rust
// backend/src/infrastructure/ista/parsers/xml_parser.rs
// Utilise quick-xml pour parser le format XML ISTA
// Structure similaire au CSV mais avec balises XML
```

---

### 3. Use Cases

#### `ISTAUseCases`
```rust
// backend/src/application/use_cases/ista_use_cases.rs

pub struct ISTAUseCases {
    reading_repo: Arc<dyn ISTAReadingRepository>,
    batch_repo: Arc<dyn ISTAImportBatchRepository>,
    mapping_repo: Arc<dyn MeterUnitMappingRepository>,
    unit_repo: Arc<dyn UnitRepository>,
    csv_parser: ISTACSVParser,
    xml_parser: ISTAXMLParser,
}

impl ISTAUseCases {
    /// Importe un fichier ISTA (CSV ou XML)
    pub async fn import_file(
        &self,
        organization_id: Uuid,
        building_id: Uuid,
        imported_by: Uuid,
        file_name: String,
        file_content: Vec<u8>,
    ) -> Result<ISTAImportBatch, String> {
        // 1. Calculer hash pour d√©duplication
        let file_hash = format!("{:x}", sha2::Sha256::digest(&file_content));

        // V√©rifier si d√©j√† import√©
        if self.batch_repo.exists_by_hash(organization_id, &file_hash).await? {
            return Err("File already imported (duplicate detected)".to_string());
        }

        // 2. Cr√©er batch
        let mut batch = ISTAImportBatch {
            id: Uuid::new_v4(),
            organization_id,
            building_id,
            imported_by,
            file_name: file_name.clone(),
            file_size: file_content.len() as i64,
            file_format: Self::detect_format(&file_name)?,
            file_hash,
            total_rows: 0,
            successful_rows: 0,
            failed_rows: 0,
            duplicate_rows: 0,
            warnings: Vec::new(),
            errors: Vec::new(),
            status: ImportStatus::Processing,
            started_at: Utc::now(),
            completed_at: None,
            created_at: Utc::now(),
        };

        // 3. Parser selon le format
        let readings = match batch.file_format {
            FileFormat::CSV => self.csv_parser.parse(&file_content, building_id, organization_id).await?,
            FileFormat::XML => self.xml_parser.parse(&file_content, building_id, organization_id).await?,
        };

        batch.total_rows = readings.len() as i32;

        // 4. Assigner batch_id aux readings
        let mut final_readings: Vec<ISTAReading> = readings.into_iter()
            .map(|mut r| { r.import_batch_id = batch.id; r })
            .collect();

        // 5. Auto-mapping via MeterUnitMapping
        self.apply_auto_mapping(&mut final_readings).await?;

        // 6. D√©tection d'anomalies
        self.detect_anomalies(&mut final_readings).await?;

        // 7. Persister les readings (transaction)
        for reading in &final_readings {
            match self.reading_repo.create(reading).await {
                Ok(_) => batch.successful_rows += 1,
                Err(e) => {
                    batch.failed_rows += 1;
                    batch.errors.push(format!("Meter {}: {}", reading.meter_number, e));
                }
            }
        }

        // 8. Finaliser batch
        batch.status = if batch.failed_rows == 0 {
            ImportStatus::Completed
        } else {
            ImportStatus::Failed
        };
        batch.completed_at = Some(Utc::now());

        self.batch_repo.create(&batch).await?;

        Ok(batch)
    }

    /// Applique les mappings compteur ‚Üí unit√© automatiquement
    async fn apply_auto_mapping(&self, readings: &mut [ISTAReading]) -> Result<(), String> {
        let mappings = self.mapping_repo.find_all_active(readings[0].building_id).await?;

        for reading in readings.iter_mut() {
            if let Some(mapping) = mappings.iter().find(|m|
                m.meter_number == reading.meter_number && m.meter_type == reading.meter_type
            ) {
                reading.unit_id = Some(mapping.unit_id);
                reading.mapped_to_unit = true;
            }
        }

        Ok(())
    }

    /// D√©tecte les anomalies de consommation
    async fn detect_anomalies(&self, readings: &mut [ISTAReading]) -> Result<(), String> {
        // Calculer la moyenne de consommation par type de compteur
        let avg_by_type = self.calculate_average_consumption(readings).await?;

        for reading in readings.iter_mut() {
            if let Some(avg) = avg_by_type.get(&reading.meter_type) {
                reading.detect_anomaly(*avg);
            }
        }

        Ok(())
    }

    /// G√©n√®re un rapport d'analyse de consommation
    pub async fn generate_consumption_report(
        &self,
        building_id: Uuid,
        period_start: DateTime<Utc>,
        period_end: DateTime<Utc>,
    ) -> Result<ConsumptionReport, String> {
        let readings = self.reading_repo
            .find_by_building_and_period(building_id, period_start, period_end)
            .await?;

        // Grouper par unit√© et par type
        let mut report = ConsumptionReport::new(building_id, period_start, period_end);

        for reading in readings {
            if let Some(consumption) = reading.consumption {
                report.add_consumption(
                    reading.unit_id,
                    reading.meter_type,
                    consumption,
                    reading.unit_of_measure.clone(),
                );
            }
        }

        Ok(report)
    }

    fn detect_format(file_name: &str) -> Result<FileFormat, String> {
        if file_name.to_lowercase().ends_with(".csv") {
            Ok(FileFormat::CSV)
        } else if file_name.to_lowercase().ends_with(".xml") {
            Ok(FileFormat::XML)
        } else {
            Err(format!("Unsupported file format: {}", file_name))
        }
    }
}
```

---

### 4. API Endpoints

#### Routes
```rust
// backend/src/infrastructure/web/routes.rs

cfg.service(
    web::scope("/api/v1")
        // ISTA Import
        .service(upload_ista_file)
        .service(list_import_batches)
        .service(get_import_batch)
        .service(list_readings_by_building)
        .service(list_readings_by_unit)
        .service(generate_consumption_report)
        .service(export_consumption_excel)
        .service(create_meter_mapping)
        .service(list_meter_mappings)
        .service(update_meter_mapping)
        .service(delete_meter_mapping)
);
```

#### Handlers
```rust
// backend/src/infrastructure/web/handlers/ista_handlers.rs

/// Upload et import d'un fichier ISTA (CSV/XML)
#[post("/ista/import")]
pub async fn upload_ista_file(
    state: web::Data<AppState>,
    user: AuthenticatedUser,
    mut payload: Multipart,
) -> impl Responder {
    // Role: Syndic, Accountant, SuperAdmin
    if !matches!(user.role.as_str(), "syndic" | "accountant" | "superadmin") {
        return HttpResponse::Forbidden().json(ErrorResponse {
            error: "Insufficient permissions".to_string(),
        });
    }

    // Extract building_id from form data
    let building_id = /* parse from multipart */;

    // Extract file content
    while let Some(item) = payload.next().await {
        let mut field = item.unwrap();
        let content_disposition = field.content_disposition();
        let file_name = content_disposition.get_filename().unwrap_or("unknown.csv").to_string();

        let mut file_content = Vec::new();
        while let Some(chunk) = field.next().await {
            file_content.extend_from_slice(&chunk.unwrap());
        }

        // Import file
        match state.ista_use_cases.import_file(
            user.organization_id,
            building_id,
            user.user_id,
            file_name,
            file_content,
        ).await {
            Ok(batch) => {
                state.audit_logger.log(AuditLogEntry::new(
                    AuditEventType::ISTAFileImported,
                    user.user_id,
                    Some(user.organization_id),
                    format!("Imported ISTA file: {} ({} rows)", batch.file_name, batch.total_rows),
                )).await;

                return HttpResponse::Ok().json(batch);
            }
            Err(e) => return HttpResponse::BadRequest().json(ErrorResponse { error: e }),
        }
    }

    HttpResponse::BadRequest().json(ErrorResponse {
        error: "No file provided".to_string(),
    })
}

/// Liste des batches d'import pour un b√¢timent
#[get("/buildings/{building_id}/ista/imports")]
pub async fn list_import_batches(
    state: web::Data<AppState>,
    user: AuthenticatedUser,
    path: web::Path<String>,
) -> impl Responder {
    let building_id = Uuid::parse_str(&path.into_inner()).unwrap();

    match state.ista_use_cases.list_batches(building_id).await {
        Ok(batches) => HttpResponse::Ok().json(batches),
        Err(e) => HttpResponse::InternalServerError().json(ErrorResponse { error: e }),
    }
}

/// Rapport de consommation pour un b√¢timent
#[get("/buildings/{building_id}/ista/consumption-report")]
pub async fn generate_consumption_report(
    state: web::Data<AppState>,
    user: AuthenticatedUser,
    path: web::Path<String>,
    query: web::Query<ConsumptionReportQuery>,
) -> impl Responder {
    let building_id = Uuid::parse_str(&path.into_inner()).unwrap();

    match state.ista_use_cases.generate_consumption_report(
        building_id,
        query.period_start,
        query.period_end,
    ).await {
        Ok(report) => HttpResponse::Ok().json(report),
        Err(e) => HttpResponse::InternalServerError().json(ErrorResponse { error: e }),
    }
}

/// Export Excel du rapport de consommation
#[get("/buildings/{building_id}/ista/consumption-report/excel")]
pub async fn export_consumption_excel(
    state: web::Data<AppState>,
    user: AuthenticatedUser,
    path: web::Path<String>,
    query: web::Query<ConsumptionReportQuery>,
) -> impl Responder {
    // G√©n√©rer rapport puis exporter vers Excel (rust_xlsxwriter)
    // Retourner fichier .xlsx en t√©l√©chargement
}

/// Cr√©er un mapping compteur ‚Üí unit√©
#[post("/ista/meter-mappings")]
pub async fn create_meter_mapping(
    state: web::Data<AppState>,
    user: AuthenticatedUser,
    dto: web::Json<CreateMeterMappingDto>,
) -> impl Responder {
    // Cr√©er mapping pour auto-mapping lors des futurs imports
}
```

---

### 5. Frontend

#### Pages Svelte
- **`ImportISTAPage.svelte`**: Upload de fichiers ISTA
- **`ISTAImportHistoryPage.svelte`**: Liste des imports avec statuts
- **`ConsumptionReportPage.svelte`**: Rapports d'analyse de consommation
- **`MeterMappingConfigPage.svelte`**: Configuration des mappings compteur ‚Üí unit√©

#### Composants
- **`ISTAFileUploader.svelte`**: Drag & drop pour upload CSV/XML
- **`ImportBatchCard.svelte`**: Carte affichant statut d'un batch (success/failed/warnings)
- **`ConsumptionChart.svelte`**: Graphiques de consommation (Chart.js)
- **`AnomalyAlert.svelte`**: Alertes pour anomalies d√©tect√©es
- **`MeterMappingTable.svelte`**: Table √©ditable pour mappings

---

## üß™ Tests

### Tests Unitaires
```rust
// backend/tests/unit/ista_reading_test.rs

#[test]
fn test_calculate_consumption() {
    let mut reading = ISTAReading::new(/* ... */).unwrap();
    reading.previous_reading_value = Some(100.0);
    reading.reading_value = 150.0;

    reading.calculate_consumption();

    assert_eq!(reading.consumption, Some(50.0));
}

#[test]
fn test_detect_anomaly_high_consumption() {
    let mut reading = ISTAReading::new(/* ... */).unwrap();
    reading.consumption = Some(200.0);

    reading.detect_anomaly(100.0); // Moyenne = 100

    assert!(reading.anomaly_detected);
    assert!(reading.anomaly_reason.is_some());
}
```

### Tests d'Int√©gration
```rust
// backend/tests/integration/ista_import_test.rs

#[tokio::test]
async fn test_import_csv_file() {
    let pool = setup_test_db().await;
    let use_cases = setup_ista_use_cases(pool.clone()).await;

    let csv_content = r#"meter_number;meter_type;location;reading_date;reading_value;previous_value;unit;estimated
12345678;water_cold;Cuisine;15/01/2025;1234.5;1200.0;m¬≥;Non
87654321;heating;Salon;15/01/2025;567;550;unit√©s;Non"#;

    let batch = use_cases.import_file(
        org_id,
        building_id,
        user_id,
        "test.csv".to_string(),
        csv_content.as_bytes().to_vec(),
    ).await.unwrap();

    assert_eq!(batch.total_rows, 2);
    assert_eq!(batch.successful_rows, 2);
    assert_eq!(batch.status, ImportStatus::Completed);
}

#[tokio::test]
async fn test_duplicate_file_rejection() {
    // Importer 2 fois le m√™me fichier
    // V√©rifier que le 2√®me import est rejet√©
}
```

### Tests E2E (BDD)
```gherkin
# backend/tests/features/ista_import.feature

Feature: Import ISTA Meter Readings
  As a Syndic
  I want to import ISTA meter readings
  So that I can track consumption history

  Scenario: Successfully import CSV file
    Given I am authenticated as a Syndic
    And I have a building with ID "building-123"
    When I upload an ISTA CSV file "readings_jan_2025.csv"
    Then the import batch should be created
    And the batch status should be "Completed"
    And 25 readings should be imported
    And meter mappings should be applied automatically

  Scenario: Detect consumption anomaly
    Given I have imported ISTA readings for the past year
    When a reading shows 200% increase compared to average
    Then an anomaly should be flagged
    And the syndic should receive an email alert
```

---

## üìö Documentation Utilisateur

### Guide d'Import ISTA

**√âtape 1: Obtenir le fichier ISTA**
- Se connecter au portail ISTA (https://www.ista.be)
- T√©l√©charger l'export annuel au format CSV ou XML
- V√©rifier que le fichier contient les colonnes requises

**√âtape 2: Configurer les mappings (premi√®re fois)**
- Aller dans `Param√®tres > Compteurs ISTA`
- Cr√©er les mappings entre num√©ros de compteur et unit√©s
- Exemple: Compteur `12345678` ‚Üí Appartement 101

**√âtape 3: Importer le fichier**
- Aller dans `B√¢timent > Import ISTA`
- Glisser-d√©poser le fichier CSV/XML
- V√©rifier le statut de l'import (succ√®s/erreurs)

**√âtape 4: Analyser les consommations**
- Aller dans `Rapports > Consommations ISTA`
- S√©lectionner la p√©riode d'analyse
- Consulter les graphiques et anomalies

---

## üîí S√©curit√© & Validation

- **Upload limit√© √† 10 MB** par fichier
- **Formats autoris√©s**: CSV, XML uniquement
- **Validation stricte** des colonnes et types de donn√©es
- **Transaction atomique**: Si 1 ligne √©choue, tout le batch √©choue (configurable)
- **Audit logging**: Tous les imports sont trac√©s (fichier, utilisateur, timestamp)
- **D√©duplication**: Hash SHA-256 du fichier pour √©viter les doublons

---

## üöÄ Migration & D√©ploiement

### Migration SQL
```sql
-- backend/migrations/20250XXX_create_ista_tables.sql

CREATE TYPE meter_type AS ENUM ('cold_water', 'hot_water', 'heating', 'calories');
CREATE TYPE file_format AS ENUM ('csv', 'xml');
CREATE TYPE import_status AS ENUM ('pending', 'processing', 'completed', 'failed');

CREATE TABLE ista_import_batches (
    id UUID PRIMARY KEY,
    organization_id UUID NOT NULL REFERENCES organizations(id),
    building_id UUID NOT NULL REFERENCES buildings(id),
    imported_by UUID NOT NULL REFERENCES users(id),

    file_name VARCHAR(255) NOT NULL,
    file_size BIGINT NOT NULL,
    file_format file_format NOT NULL,
    file_hash VARCHAR(64) NOT NULL, -- SHA-256

    total_rows INTEGER NOT NULL DEFAULT 0,
    successful_rows INTEGER NOT NULL DEFAULT 0,
    failed_rows INTEGER NOT NULL DEFAULT 0,
    duplicate_rows INTEGER NOT NULL DEFAULT 0,
    warnings TEXT[],
    errors TEXT[],

    status import_status NOT NULL DEFAULT 'pending',
    started_at TIMESTAMP WITH TIME ZONE NOT NULL,
    completed_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),

    UNIQUE(organization_id, file_hash) -- D√©duplication
);

CREATE TABLE ista_readings (
    id UUID PRIMARY KEY,
    organization_id UUID NOT NULL REFERENCES organizations(id),
    building_id UUID NOT NULL REFERENCES buildings(id),
    unit_id UUID REFERENCES units(id), -- NULL si pas mapp√©
    import_batch_id UUID NOT NULL REFERENCES ista_import_batches(id) ON DELETE CASCADE,

    meter_number VARCHAR(100) NOT NULL,
    meter_type meter_type NOT NULL,
    meter_location VARCHAR(255),

    reading_date TIMESTAMP WITH TIME ZONE NOT NULL,
    reading_value DOUBLE PRECISION NOT NULL,
    previous_reading_value DOUBLE PRECISION,
    consumption DOUBLE PRECISION,
    unit_of_measure VARCHAR(20) NOT NULL,

    ista_contract_number VARCHAR(100),
    billing_period_start TIMESTAMP WITH TIME ZONE,
    billing_period_end TIMESTAMP WITH TIME ZONE,
    estimated BOOLEAN NOT NULL DEFAULT false,

    mapped_to_unit BOOLEAN NOT NULL DEFAULT false,
    anomaly_detected BOOLEAN NOT NULL DEFAULT false,
    anomaly_reason TEXT,

    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

CREATE TABLE meter_unit_mappings (
    id UUID PRIMARY KEY,
    organization_id UUID NOT NULL REFERENCES organizations(id),
    building_id UUID NOT NULL REFERENCES buildings(id),
    unit_id UUID NOT NULL REFERENCES units(id),

    meter_number VARCHAR(100) NOT NULL,
    meter_type meter_type NOT NULL,
    meter_location VARCHAR(255),

    fallback_reference VARCHAR(100),

    active BOOLEAN NOT NULL DEFAULT true,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    deactivated_at TIMESTAMP WITH TIME ZONE,

    UNIQUE(organization_id, meter_number, meter_type) -- Un compteur = une unit√©
);

-- Index pour performance
CREATE INDEX idx_ista_readings_building ON ista_readings(building_id, reading_date);
CREATE INDEX idx_ista_readings_unit ON ista_readings(unit_id, reading_date);
CREATE INDEX idx_ista_readings_meter ON ista_readings(meter_number, meter_type);
CREATE INDEX idx_ista_batches_org ON ista_import_batches(organization_id, created_at DESC);
CREATE INDEX idx_meter_mappings_active ON meter_unit_mappings(building_id, active);
```

---

## üìä √âvolutions Futures

### Phase 3 (K8s - Real-time)
- **Alertes temps r√©el** pour anomalies d√©tect√©es
- **API ISTA automatis√©e** (si ISTA fournit API)
- **Pr√©visions de consommation** (ML) bas√©es sur l'historique
- **Int√©gration IoT** (Issue #030) pour validation crois√©e relev√©s ISTA vs capteurs temps r√©el

### Int√©gration avec Issue #028 (Commande Group√©e √ânergie)
- Utiliser l'historique ISTA pour **estimer la consommation future** lors des appels d'offres
- Comparer les **√©conomies r√©alis√©es** apr√®s changement de fournisseur

---

## ‚úÖ Checklist de Compl√©tion

- [ ] Entit√©s Domain cr√©√©es et test√©es
- [ ] Parsers CSV et XML impl√©ment√©s
- [ ] Use Cases avec logique m√©tier compl√®te
- [ ] Repositories PostgreSQL avec migrations
- [ ] API endpoints avec authentification
- [ ] Frontend: Pages et composants Svelte
- [ ] Tests unitaires (100% couverture entities)
- [ ] Tests d'int√©gration (import, mapping, rapports)
- [ ] Tests BDD (sc√©narios utilisateur)
- [ ] Documentation utilisateur (guide d'import)
- [ ] Migration SQL valid√©e
- [ ] Logs d'audit int√©gr√©s
- [ ] D√©ploiement en staging et validation QA

---

**Responsable**: √Ä assigner
**Milestone**: Phase 2 - K3s Automation
**Estimation**: 6-8h
**D√©pendances**: Aucune (standalone), synergie avec Issue #030 (IoT)
